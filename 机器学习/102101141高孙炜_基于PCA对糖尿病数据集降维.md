[TOC]



# 0. 知识准备

![image-20240521171809162](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521171809162.png)

# 1.导入包

```python
# 导入包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

# 2.导入数据集

```python
# 导入数据集
dataset = pd.read_csv('/home/dataset/pima-indians-diabetes.csv')
dataset
```

![image-20240521171932213](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521171932213.png)

# 3. 数据预处理

## 3.1 检测缺失值

```python
# 检测缺失值
null_df = dataset.isnull().sum()
null_df
```

![image-20240521172006920](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521172006920.png)

## 3.2 生成自变量和因变量

```python
# 生成自变量和因变量
X = dataset.iloc[:,0:8].values
y = dataset.iloc[:,8].values
```

## 3.3 拆分训练集和测试集

```python
# 拆分训练集和测试集
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
```

![image-20240521172113271](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521172113271.png)

## 3.4 特征缩放

```python
# 特征缩放
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
```



# 4 PCA降维

## 4.1 使用PCA生成新的自变量

```python
# 使用 PCA 生成新的自变量
from sklearn.decomposition import PCA
pca = PCA(n_components = None) # 不指定特征的个数
X_train_pca = pca.fit_transform(X_train)
print(X_train_pca)
```

```python
X_test_pca = pca.transform(X_test)
```

```python
print(X_test.shape)
print(X_test_pca.shape)
```

![image-20240521172342287](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521172342287.png)

## 4.2 验证PCA转换规则

### 4.2.1 打印旧的自变量和新的自变量的转换系数

```python
# 打印旧的自变量与新的自变量的转换系数
print('打印旧的自变量与新的自变量的转换系数是：\n', pca.components_)
```

![image-20240521172622478](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521172622478.png)

### 4.2.2 增加转换系数的可读性

```
# 增加转换系数的可读性
old_columns = list(dataset)[:-1]
new_columns = ['pc' + str(i) + '_component' for i in range(X_train.shape[1])]
components_df = pd.DataFrame(pca.components_, columns = old_columns, index = new_columns)
components_df = components_df.T # 转置，增加可读性
print('打印旧的自变量与新的自变量的转换系数是：\n', components_df)
```

![image-20240521172856942](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521172856942.png)

### 4.2.3 检验X——train_pca的由来

```python
components = components_df.values
print(components.shape)
```

![image-20240521172956196](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521172956196.png)

```python
# 检验x_train_pca的由来
verify_matrix = X_train.dot(components)
print(verify_matrix)
```

![image-20240521173020287](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521173020287.png)

```python
print(X_train_pca)
```

![image-20240521173052141](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521173052141.png)

## 4.3 选择PCA个数

### 4.3.1 打印PCA的方差解释比率

```python
# 打印 pca 的方差解释比率
print('PCA的方差解释比率是：\n', pca.explained_variance_ratio_)
```

![image-20240521173145243](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521173145243.png)

### 4.3.2 画出新的自变量个数 VS 累计方差解释

```python
# 画出新的自变量的个数 VS 累计方差解释
plt.plot([i for i in range(1, X_train.shape[1] + 1)], np.cumsum(pca.explained_variance_ratio_), c='orange')
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.show()
```

![image-20240521173226299](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521173226299.png)

## 4.4 使用PCA降维

```python
# 使用 PCA 降维
pca = PCA(n_components = 6) # 6由上一步选出
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)
print(X_train_pca)
```

![image-20240521173256532](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521173256532.png)

## 4.5 可视化PCA降维效果

```python
from matplotlib.colors import ListedColormap
X_set, y_set = X_train_pca, y_train
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                color = ListedColormap(('red', 'green'))(i), label = j)
plt.title('PCA Viz')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()
```



![image-20240521173316983](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521173316983.png)

# 5. 分别在原始数据集和降维后的数据集上训练模型

## 5.1 在原始数据集上训练模型

```python
# 构建模型
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(penalty='l2', C=1, class_weight='balanced', random_state = 0)
classifier.fit(X_train, y_train)
# 预测测试集
y_pred = classifier.predict(X_test)
# 评估模型性能
from sklearn.metrics import recall_score
print(recall_score(y_test, y_pred))
```

![image-20240521173520275](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521173520275.png)

## 5.2 在降维后的数据集上训练模型

```python
# 构建模型
classifier = LogisticRegression(penalty='l2', C=1, class_weight='balanced', random_state = 0)
classifier.fit(X_train_pca, y_train)
# 预测测试集
y_pred = classifier.predict(X_test_pca)
# 评估模型性能
print(recall_score(y_test, y_pred))
```

![image-20240521173558372](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521173558372.png)

# 6. 实验结论

#### 1. 可以通过主成分分析法做特征选择（降维）

在本实验中，我们探讨了主成分分析法（Principal Component Analysis, PCA）作为特征选择和降维工具对模型性能的影响。PCA是一种经典的降维技术，通过线性变换将高维数据映射到低维空间，同时尽量保留原始数据的方差信息。

- **特征选择与降维**：PCA通过构造一组新的特征（主成分），这些主成分是原始特征的线性组合，且彼此正交。实验表明，使用PCA可以有效地减少数据维度，消除冗余特征，降低模型复杂度。
- **方差解释**：PCA保留了原始数据中最大的方差信息，即主成分中包含了原始数据的主要信息。在本实验中，通过选择适当数量的主成分，我们能够在降维的同时尽可能保留原始数据的重要特征。

#### 2. 降维后，模型性能可能会提升，也可能会下降（取决于降维方法）

通过实验分析，我们发现降维对模型性能的影响并不固定，具体效果取决于降维方法以及数据的特性。

- **性能提升**：
  - **降噪**：PCA通过消除冗余和相关性强的特征，可以有效降低数据噪声，提高模型的泛化能力。实验结果显示，在某些数据集上，经过PCA降维后，模型性能得到了显著提升。
  - **计算效率**：降维减少了特征数量，从而降低了模型的计算复杂度，提高了训练和预测速度。实验表明，尤其在高维数据集上，PCA降维能够显著提升模型的计算效率，减少内存和时间开销。
- **性能下降**：
  - **信息损失**：PCA在降维过程中可能会丢失部分原始数据的重要信息，尤其是当选择的主成分数量较少时，这种信息损失可能导致模型性能下降。实验结果显示，在某些情况下，降维后的模型表现不如原始模型。
  - **特征非线性关系**：PCA是一种线性降维方法，对于非线性特征关系复杂的数据，PCA可能无法有效捕捉数据中的非线性模式，从而影响模型性能。实验表明，对于某些高度非线性的数据集，PCA降维后的模型性能不如原始模型。

### 结论总结

通过本实验，我们验证了主成分分析法（PCA）作为特征选择和降维工具在提高模型性能和计算效率方面的潜力。同时，实验结果也表明，降维对模型性能的影响因数据特性和降维方法的选择而异。

在应用PCA进行降维时，应根据具体数据集的特性和模型需求，合理选择主成分数量，平衡信息保留和降维效果。通过综合考虑降维的优劣，可以在不同的应用场景中实现模型性能的优化。

这些实验结论为我们在模型构建和优化过程中提供了重要参考。通过适当的特征选择和降维策略，可以有效提升模型性能，适应不同的数据特性和应用需求。