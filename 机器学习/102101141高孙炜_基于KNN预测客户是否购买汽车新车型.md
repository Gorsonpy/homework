[TOC]



# 0. 背景知识

询问chatgpt相关知识:

![image-20240521111417547](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521111417547.png)

# 1.导入包

```python
# 导入包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

# 2. 导入数据集

```python
dataset = pd.read_csv('/home/dataset/Social_Network_Ads.csv')
dataset
```

![image-20240521111746201](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521111746201.png)

# 3. 数据预处理

## 3.1 检测缺失值

```python
# 检测缺失值
blank = dataset.isnull().sum()
blank
```

![image-20240521111733470](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521111733470.png)

## 3.2 生成自变量和因变量

```python
# 生成自变量和因变量
X = dataset.iloc[:, [2, 3]].values
X[:5, :]
```

![image-20240521111758645](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521111758645.png)

```python
y = dataset.iloc[:, 4].values
y[:5]
```

![image-20240521111835542](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521111835542.png)

## 3.3 查看样本是否均衡

```python
# 查看样本是否均衡
sample_0 = sum(dataset['Purchased']==0)
sample_1 = sum(dataset['Purchased']==1)
print('不买车的样本占总样本的%.2f' %(sample_0/(sample_0 + sample_1)))
```

![image-20240521111919189](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521111919189.png)

## 3.4 把数据拆分成训练集和测试集

```python
# 将数据拆分成训练集和测试集
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
```

![image-20240521111957747](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521111957747.png)

## 3.5 特征缩放

```python
# 特征缩放
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
```

# 4. 使用不同的参数 构建KNN模型

## 4.1 模型1

### 4.1.1 构建KNN模型并训练

```python
# 使用不同的参数构建KNN模型
# 模型1：构建KNN模型并训练模型（n_neighbors = 5, weights='uniform', metric = 'minkowski', p = 2）
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5, weights='uniform', metric = 'minkowski', p = 2)
classifier.fit(X_train, y_train)
```

### 4.1.2 预测测试集

```python
# 预测测试集
y_pred = classifier.predict(X_test)
y_pred[:5]
```

![image-20240521112527326](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521112527326.png)

### 4.1.3 生成混淆矩阵

```python
# 生成混淆矩阵
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)
```



![image-20240521112646256](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521112646256.png)

### 4.1.4 可视化

```python
# 可视化测试集的预测结果
from matplotlib.colors import ListedColormap
plt.figure()
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('pink', 'limegreen')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                color = ListedColormap(('red', 'green'))(i), label = j)
plt.title('KNN (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()
```

![image-20240521112723515](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521112723515.png)

### 4.1.5 评估模型性能

调用函数库的办法：

```python
# 评估模型性能
from sklearn.metrics import accuracy_score
print(accuracy_score(y_test, y_pred))
```

![image-20240521112800413](C:\Users\gorsonpy\AppData\Roaming\Typora\typora-user-images\image-20240521112800413.png)

使用混淆矩阵的办法：

```
(cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])
```



## 4.2 模型2

```python
# 模型2：构建KNN模型并训练模型（n_neighbors = 3, weights='distance', metric = 'minkowski', p = 1）
classifier = KNeighborsClassifier(n_neighbors = 3, weights='distance', metric = 'minkowski', p = 1)
classifier.fit(X_train, y_train)
# 预测测试集
y_pred = classifier.predict(X_test)
y_pred[:5]
# 生成混淆矩阵
cm = confusion_matrix(y_test, y_pred)
print(cm)
# 可视化测试集的预测结果
plt.figure()
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('pink', 'limegreen')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                color = ListedColormap(('red', 'green'))(i), label = j)
plt.title('KNN (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()
# 评估模型性能
print(accuracy_score(y_test, y_pred))

```

# 5 实验结论

### 实验结论

#### 1. KNN可以进行非线性分类

在本实验中，我们验证了KNN算法在处理非线性分类任务中的有效性。通过实验，我们发现KNN可以根据样本之间的相似性，在复杂的非线性数据分布中准确地进行分类。这是因为KNN不依赖于数据的线性可分性，而是利用距离度量和邻近样本的标签信息进行分类，从而能够适应各种形状和模式的决策边界。

实验结果表明，在使用KNN进行非线性分类时，即使数据的决策边界非常复杂，KNN也能够通过适当选择邻居数量K，找到合理的分类边界，获得较高的分类精度。具体而言，当K值适中时，KNN能够平滑决策边界，减少噪声影响，提高分类准确性。

#### 2. 不同超参数对KNN模型性能的影响

KNN模型的性能在很大程度上依赖于超参数的选择，尤其是邻居数量K和距离度量方式。通过实验分析，我们总结了以下几点结论：

- **邻居数量K的影响**：
  - **K值过小**：当K值较小时，模型容易受到噪声和异常值的影响，导致过拟合。这是因为少数邻居可能会包含异常样本，从而使分类结果偏向这些异常点。
  - **K值过大**：当K值较大时，模型的决策边界过于平滑，可能导致欠拟合。由于考虑的邻居样本过多，一些重要的局部模式可能被忽略，从而降低分类精度。
  - **最佳K值**：实验表明，存在一个最佳的K值范围，使得模型能够在平滑决策边界和保持局部模式之间取得平衡，从而实现最优的分类性能。通常通过交叉验证来确定最佳K值。
- **距离度量方式的影响**：
  - **欧氏距离**：常用于连续变量，适用于大多数情况下的KNN分类任务。实验表明，欧氏距离在处理空间均匀分布的数据时表现良好。
  - **曼哈顿距离**：在某些高维数据或城市街区网格状数据中表现较好。实验发现，曼哈顿距离在某些特定应用场景中优于欧氏距离。
  - **闵可夫斯基距离**：通过调节参数p，能够在欧氏距离和曼哈顿距离之间切换，为KNN模型提供了更多灵活性。实验结果显示，不同的p值会显著影响分类性能，需根据具体数据特点进行选择。
- **权重策略的影响**：
  - **统一权重**：所有邻居样本具有相同的权重，适用于数据分布均匀的情况。
  - **距离加权**：根据邻居样本与待分类样本的距离赋予不同权重，距离越近权重越大，适用于考虑邻近样本更具代表性的数据分布。实验表明，距离加权策略在处理非均匀分布数据时能有效提高分类精度。

通过一系列实验，我们确认了KNN模型在非线性分类任务中的有效性，同时详细分析了不同超参数对模型性能的影响。最佳的K值、合适的距离度量方式和权重策略能够显著提升KNN模型的分类性能。未来的研究可以进一步探索KNN在不同应用场景中的优化方法，为更复杂的分类任务提供更强大的解决方案。